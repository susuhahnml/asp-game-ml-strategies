\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{berlekamp_winning_2001}
\citation{bouton_nim_1901}
\citation{pisano_combinatorial_2015}
\citation{schockaert_combinatorial_2016}
\citation{pisano_combinatorial_2015}
\@writefile{toc}{\contentsline {section}{\numberline {1}Theoretical Background: Computational Problem Solving and Combinatorial Games}{3}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}What are combinatorial games?}{3}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The relationship of combinatorial games and artificial intelligence}{3}{subsection.1.2}}
\citation{hutchison_efficient_2007}
\citation{silver_mastering_2016}
\citation{silver_mastering_2017}
\citation{lloyd_practical_1994}
\citation{lloyd_practical_1994}
\citation{love_general_2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Model dynamics and declarative programming languages}{4}{subsection.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Project Goal: Creation of an algorithm capable of playing dominoes using reinforcement learning}{5}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Why dominoes?}{5}{subsection.2.1}}
\citation{russell_artificial_2010}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A so called ‘double-six’ domino set with which is the most common variant of dominoes is played.\relax }}{6}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dom_set}{{1}{6}{A so called ‘double-six’ domino set with which is the most common variant of dominoes is played.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Why Reinforcement Learning?}{6}{subsection.2.2}}
\newlabel{sec:rl}{{2.2}{6}{Why Reinforcement Learning?}{subsection.2.2}{}}
\citation{mnih_playing_2013}
\citation{francois-lavet_introduction_2018}
\citation{wiering_reinforcement_2012}
\citation{love_general_2008}
\@writefile{toc}{\contentsline {section}{\numberline {3}Project Execution}{7}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Formalisation}{7}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Game Description Language (GDL) and Answer Set Programming (ASP)}{7}{subsubsection.3.1.1}}
\citation{love_general_2008}
\citation{gebser_clingo_2014}
\citation{lifschitz_answer_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Game Encoding and Vector Representation for Dominoes}{8}{subsubsection.3.1.2}}
\newlabel{sec:game_encoding}{{3.1.2}{8}{Game Encoding and Vector Representation for Dominoes}{subsubsection.3.1.2}{}}
\citation{francois-lavet_introduction_2018}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An arbitrary game state and a transition following an action represented visually and with their corresponding one-hot-encoded vectors.\relax }}{9}{figure.caption.3}}
\newlabel{fig:encoding}{{2}{9}{An arbitrary game state and a transition following an action represented visually and with their corresponding one-hot-encoded vectors.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Application of Q-Learning}{9}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Algorithm Selection}{9}{subsubsection.3.2.1}}
\citation{russell_artificial_2010}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces General depiction of a Markov Decision Process in which an agent interacts with an environment through actions based on a state observed in the environment. The environment also provides a reward to the agent.\relax }}{10}{figure.caption.4}}
\newlabel{fig:mdp}{{3}{10}{General depiction of a Markov Decision Process in which an agent interacts with an environment through actions based on a state observed in the environment. The environment also provides a reward to the agent.\relax }{figure.caption.4}{}}
\citation{sutton_reinforcement_2018}
\citation{russell_artificial_2010}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Implementation}{13}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Benchmarking and results}{13}{subsubsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Benchmarking results for DQN agents trained against a random and a strategic player. Benchmarking was performed every 1000 training iterations and the percentages of games won and games lost with an illegal action were recorded.\relax }}{14}{figure.caption.5}}
\newlabel{fig:dqn_both}{{4}{14}{Benchmarking results for DQN agents trained against a random and a strategic player. Benchmarking was performed every 1000 training iterations and the percentages of games won and games lost with an illegal action were recorded.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Resulting project modifications}{14}{subsubsection.3.2.4}}
\citation{olivas_transfer_2010}
\citation{olivas_transfer_2010}
\citation{olivas_transfer_2010}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Three common ways in which learning a target task can benefit from a previously learned source task as shown in \citet  {olivas_transfer_2010}\relax }}{15}{figure.caption.6}}
\newlabel{fig:trnsfr_bttr}{{5}{15}{Three common ways in which learning a target task can benefit from a previously learned source task as shown in \citet {olivas_transfer_2010}\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Application of Deep Supervised Transfer Learning}{15}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Deep Supervised Transfer Learning}{15}{subsubsection.3.3.1}}
\citation{silver_mastering_2017}
\citation{browne_survey_2012}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Data Generation with Monte Carlo Tree Search}{16}{subsubsection.3.3.2}}
\citation{browne_survey_2012}
\citation{browne_survey_2012}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  One iteration of the Monte Carlo Tree Search algorithm as shown in \citet  {browne_survey_2012}\relax }}{17}{figure.caption.7}}
\newlabel{fig:mcts}{{6}{17}{One iteration of the Monte Carlo Tree Search algorithm as shown in \citet {browne_survey_2012}\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Histogram of move probabilities obtained via Monte Carlo Tree Search\relax }}{18}{figure.caption.8}}
\newlabel{fig:hist_labels}{{7}{18}{Histogram of move probabilities obtained via Monte Carlo Tree Search\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Model Setup}{18}{subsubsection.3.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Accuracy and loss curves for the source task of learning the game dynamics of through one-hot encoded state and action vectors.\relax }}{19}{figure.caption.9}}
\newlabel{fig:src_tsk}{{8}{19}{Accuracy and loss curves for the source task of learning the game dynamics of through one-hot encoded state and action vectors.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Benchmarking and Results}{19}{subsubsection.3.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Accuracy and loss curves of a selected model for the target task predicting move probability through one-hot encoded state and action vectors. The area surrounded by the red border inside the accuracy graph is a zoomed-in view of the first 50 iterations. \relax }}{20}{figure.caption.10}}
\newlabel{fig:trnsfr_both}{{9}{20}{Accuracy and loss curves of a selected model for the target task predicting move probability through one-hot encoded state and action vectors. The area surrounded by the red border inside the accuracy graph is a zoomed-in view of the first 50 iterations. \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Mean loss for models with different batch sizes and learning rates.\relax }}{21}{figure.caption.11}}
\newlabel{fig:means_aggr}{{10}{21}{Mean loss for models with different batch sizes and learning rates.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Mean loss and mean number of epochs of the models fitted in the grid search aggregated over the factors transfer learning and model depth.\relax }}{21}{figure.caption.12}}
\newlabel{fig:grdsrch}{{11}{21}{Mean loss and mean number of epochs of the models fitted in the grid search aggregated over the factors transfer learning and model depth.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}Discussion}{21}{subsubsection.3.3.5}}
\citation{silver_mastering_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Implementation of AlphaGo}{22}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}The AlphaGo Zero algorithm}{22}{subsubsection.3.4.1}}
\citation{silver_mastering_2017}
\citation{silver_mastering_2017}
\citation{silver_mastering_2017}
\citation{silver_mastering_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Self play performed by AlphaGo Zero depicted in \citet  {silver_mastering_2017}\relax }}{23}{figure.caption.13}}
\newlabel{fig:selfplay}{{12}{23}{Self play performed by AlphaGo Zero depicted in \citet {silver_mastering_2017}\relax }{figure.caption.13}{}}
\citation{silver_mastering_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Model (re-)training in AlphaGo Zero as depicted in \citet  {silver_mastering_2017}\relax }}{24}{figure.caption.14}}
\newlabel{fig:retraining}{{13}{24}{Model (re-)training in AlphaGo Zero as depicted in \citet {silver_mastering_2017}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Specific implementation}{24}{subsubsection.3.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Benchmarking results for AlphaGo Zero agents trained with and without penalisation of invalid moves. Benchmarking was performed every time the retrained model replaced the previous model.\relax }}{25}{figure.caption.15}}
\newlabel{fig:alphazero}{{14}{25}{Benchmarking results for AlphaGo Zero agents trained with and without penalisation of invalid moves. Benchmarking was performed every time the retrained model replaced the previous model.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Benchmarking and Results}{25}{subsubsection.3.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Discussion}{25}{subsubsection.3.4.4}}
\bibstyle{apalike}
\bibdata{domiknows}
\bibcite{berlekamp_winning_2001}{{1}{2001}{{Berlekamp et~al.}}{{}}}
\bibcite{bouton_nim_1901}{{2}{1901}{{Bouton}}{{}}}
\bibcite{browne_survey_2012}{{3}{2012}{{Browne et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{26}{section.4}}
\@writefile{toc}{\contentsline {section}{References}{26}{section.4}}
\bibcite{hutchison_efficient_2007}{{4}{2007}{{Coulom}}{{}}}
\bibcite{schockaert_combinatorial_2016}{{5}{2016}{{Duchêne}}{{}}}
\bibcite{francois-lavet_introduction_2018}{{6}{2018}{{Francois-Lavet et~al.}}{{}}}
\bibcite{gebser_clingo_2014}{{7}{2014}{{Gebser et~al.}}{{}}}
\bibcite{lifschitz_answer_2019}{{8}{2019}{{Lifschitz}}{{}}}
\bibcite{lloyd_practical_1994}{{9}{1994}{{Lloyd}}{{}}}
\bibcite{love_general_2008}{{10}{2008}{{Love et~al.}}{{}}}
\bibcite{mnih_playing_2013}{{11}{2013}{{Mnih et~al.}}{{}}}
\bibcite{pisano_combinatorial_2015}{{12}{2015}{{Rougetet}}{{}}}
\bibcite{russell_artificial_2010}{{13}{2010}{{Russell et~al.}}{{}}}
\bibcite{silver_mastering_2016}{{14}{2016}{{Silver et~al.}}{{}}}
\bibcite{silver_mastering_2017}{{15}{2017}{{Silver et~al.}}{{}}}
\bibcite{sutton_reinforcement_2018}{{16}{2018}{{Sutton and Barto}}{{}}}
\bibcite{wiering_reinforcement_2012}{{17}{2012}{{Szita}}{{}}}
\bibcite{olivas_transfer_2010}{{18}{2010}{{Torrey and Shavik}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Rules for Nim}{28}{appendix.A}}
\newlabel{a:nimrules}{{A}{28}{Rules for Nim}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Rules for Two-Player Dominoes without a draw pile}{29}{appendix.B}}
\newlabel{a:domrules}{{B}{29}{Rules for Two-Player Dominoes without a draw pile}{appendix.B}{}}
