\section{Introduction}

In the last decades, significant advancements have been made in optimizing artificially intelligent computer agents in two-player games such as \textit{Chess} and \textit{Go} \citep{Silver1140}. Much of this success can be attributed to advancements in deep reinforcement learning and corresponding GPU hardware-acceleration \citep{Silver1140}. However, one common disadvantage of these techniques is the lack of explainability of the strategies learned; which is largely due to the black-box nature of deep learning models \citep{DBLP:journals/corr/abs-1804-02477}. These models are specially designed for games with a high branching factor which makes it impossible to explore the complete game space to make a decision. Most of these systems rely on Monte Carlo Tree Search and Reinforcement learning techniques to avoid the construction of a full search tree. In the case of smaller games, where we are able to compute the full tree search for smaller instances, more algorithms become available to perform the analysis.

In this project, we aim to learn game strategies for two common small games, specifically \textit{Nim}, and \textit{Tic-Tac-Toe}. We use formalisms from Game Description Language (GDL) \citep{love@ggp} to encode the rules of the games in Answer Set Programming (ASP)\citep{GlimpsASP}, and simulate the gameplay using the ASP system \textit{clingo}. We implement various techniques for learning game strategies; a brute force \textit{Minimax} algorithm, a specialized Minimax algorithm employing the optimization statements provided by \textit{clingo} and we also use Inductive Learning of Answer Set Programs (\textit{ILASP}) to learn weak constraints from positive ordered examples. Finally, we benchmark the various learning techniques against one another by treating them as independent agents to evaluate their performance and scalability. Our method represents a novel technique since we attempt to extend the utility of ASP from its traditional single-agent problem-solving setting to that of a dynamic dual-agent simulation setting.

In sections \hyperref[back_con]{2} and \hyperref[methods]{3}, we describe the aforementioned background concepts and methodologies respectively. In section \hyperref[results]{4}, we describe the results of our methodologies and then in section \hyperref[discussion]{5} proceed to discuss pertinent limitations of the tested methodologies. Finally, we conclude our research in section \hyperref[conclusions]{6}.